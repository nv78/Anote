{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "import requests\n",
    "import chromadb\n",
    "import openai\n",
    "from chromadb.config import Settings\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "import fitz  # PyMuPDF\n",
    "from chromadb.utils import embedding_functions\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import base64\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from bert_score import score\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import base64\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from bert_score import score\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "\n",
    "def extract_pdf_url(url):\n",
    "    \"\"\"\n",
    "    Extracts the actual PDF URL from the given URL.\n",
    "    Decodes it from base64 if necessary.\n",
    "    \"\"\"\n",
    "    if url.lower().endswith('.pdf'):\n",
    "        return url  # Direct PDF URL\n",
    "    else:\n",
    "        parsed_url = urlparse(url)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        pdf_target = query_params.get('pdfTarget', [None])[0]\n",
    "\n",
    "        if pdf_target:\n",
    "            pdf_url = base64.b64decode(pdf_target).decode('utf-8')\n",
    "            return pdf_url\n",
    "        else:\n",
    "            raise ValueError(\"No valid PDF URL found in the provided URL\")\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from a given URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_url = extract_pdf_url(url)\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "\n",
    "        print(f\"Downloaded PDF from: {pdf_url} to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading PDF: {e}\")\n",
    "        \n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_hub(pdf_path, doc_name):\n",
    "    # Normalize doc_name to create a valid directory name\n",
    "    normalized_doc_name = doc_name.replace(' ', '_').replace('/', '_')\n",
    "    \n",
    "    # Define the base directory for vector databases\n",
    "    base_dir = \"vector_db\"\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key=openai_api_key)\n",
    "    # Ensure the base directory exists\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "\n",
    "    # Define the full path to the specific database directory\n",
    "    db_directory = os.path.join(base_dir, \"db_\" + normalized_doc_name)\n",
    "    embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "    # Check if the database directory already exists\n",
    "    if os.path.exists(db_directory):\n",
    "        print(f\"Using existing database for document: {doc_name}\")\n",
    "        vectordb = Chroma(persist_directory=db_directory, embedding_function=embeddings)\n",
    "    else:\n",
    "        print(f\"Creating new database for document: {doc_name}\")\n",
    "\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        splitter = CharacterTextSplitter(\n",
    "            chunk_size=1300, \n",
    "            chunk_overlap=5,\n",
    "            #separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            length_function=len)\n",
    "        texts = splitter.split_documents(documents)\n",
    "        print(len(texts))\n",
    "        vectordb = Chroma.from_documents(\n",
    "            texts, \n",
    "            embeddings,  \n",
    "            persist_directory=db_directory,\n",
    "            )\n",
    "\n",
    "        vectordb.persist()\n",
    "\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai_with_context(query, vectordb, openai_api_key, top_k=5):\n",
    "    # Initialize Chroma client\n",
    "\n",
    "    #context = vectordb.similarity_search(query, k = 2)\n",
    "    context = vectordb.as_retriever()\n",
    "    # retrieved_docs = vectordb.similarity_search(query, k=2)\n",
    "    # print(retrieved_docs[0].page_content)\n",
    "    # context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "    print(context)\n",
    "\n",
    "    #openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key=openai_api_key)\n",
    "    \n",
    "    template = \"\"\"You are a financial chatbot trained to answer questions based on the information provided in 10-K\n",
    "    documents. Your responses should be directly sourced from the content of these documents. When asked\n",
    "    a question, ensure that your answer is explicitly supported by the text in the 10-K filing, and do not\n",
    "    include any external information, interpretations, or assumptions not clearly stated in the document. If\n",
    "    a question pertains to financial data or analysis that is not explicitly covered in the 10-K filing provided,\n",
    "    respond by stating that the information is not available in the document. Your primary focus should\n",
    "    be on accuracy, specificity, and adherence to the information in 10-K documents, particularly regarding\n",
    "    financial statements, company performance, and market position.\"\"\"\n",
    "    \n",
    "    prompt = f\"\\nContext:\\n{context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": template},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    response = get_assistant_response(messages)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chat_history(messages):\n",
    "    for message in messages:\n",
    "        print(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "\n",
    "def get_assistant_response(messages):\n",
    "  client = OpenAI(api_key = openai_api_key)\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages],\n",
    "  )\n",
    "\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    # Create a TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the two texts\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    # Extract the similarity score\n",
    "    similarity_score = cosine_sim[0][0]\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "def calculate_bertscore(candidate, reference):\n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    return P.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_responses(question, model_answer, refrence_answer):\n",
    "    for question in question:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that provides concise and accurate answers.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        response = model_answer\n",
    "\n",
    "    evaluation_scores = []\n",
    "    for i in range(len(response)):\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Evaluate the following response against the reference answer. Assign a score between 0 and 1 based on correctness and provide a brief justification.\n",
    "\n",
    "        Question: {question}\n",
    "        Response: {model_answer}\n",
    "        Reference Answer: {refrence_answer}\n",
    "\n",
    "        Score (0 to 1):\n",
    "        Justification:\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an evaluator that scores responses based on correctness.\"},\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "        ]\n",
    "        evaluation_response = get_assistant_response(messages)\n",
    "\n",
    "        evaluation_text = evaluation_response.strip()\n",
    "        try:\n",
    "            score_line = evaluation_text.split('\\n')[0]\n",
    "            score = float(score_line.split(':')[1].strip())\n",
    "            evaluation_scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing score: {e}\")\n",
    "            evaluation_scores.append(0.0)\n",
    "\n",
    "    average_score = sum(evaluation_scores) / len(evaluation_scores) if evaluation_scores else 0\n",
    "    print(f'Average Correctness Score: {average_score:.2f}')\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    dataset = load_dataset(\"PatronusAI/financebench\")\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    test = df[:5]\n",
    "    results_list = []\n",
    "\n",
    "    for index, row in test.iterrows():\n",
    "        download_dir = \"pdf_documents\"\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        doc_link = row['doc_link']\n",
    "        doc_name = row['doc_name']\n",
    "        question = row['question']\n",
    "        ref_answer = row['answer']\n",
    "        ref_context = row['evidence_text']\n",
    "        doc_path = os.path.join(download_dir, f\"{doc_name}.pdf\")\n",
    "\n",
    "        download_pdf(doc_link, doc_path)\n",
    "        vector_db = create_knowledge_hub(doc_path, doc_name)\n",
    "        print(\"Querying Model now\")\n",
    "        model_answer = query_openai_with_context(question, vector_db, openai_api_key = openai_api_key)\n",
    "        print(model_answer)\n",
    "\n",
    "        # Evaluation for structured QA \n",
    "        cosine_similarity_score = calculate_cosine_similarity(model_answer, ref_answer)\n",
    "        bert_score = calculate_bertscore(model_answer, ref_answer)\n",
    "        llm_eval = evaluate_llm_responses(question, model_answer, ref_answer)\n",
    "\n",
    "        # Append results to the list\n",
    "        results_list.append({\n",
    "            'doc_name': doc_name,\n",
    "            'question': question,\n",
    "            'ref_answer': ref_answer,\n",
    "            'model_answer': model_answer,\n",
    "            'cosine_similarity': cosine_similarity_score,\n",
    "            'bert_score': bert_score,\n",
    "            'llm_eval': llm_eval\n",
    "        })\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded PDF from: https://investors.3m.com/financials/sec-filings/content/0001558370-19-000470/0001558370-19-000470.pdf to pdf_documents\\3M_2018_10K.pdf\n",
      "Using existing database for document: 3M_2018_10K\n",
      "Querying Model now\n",
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027B1CFA2A50>\n",
      "The FY2018 capital expenditure amount for 3M was approximately $1.417 billion as per the Consolidated Statements of Cash Flows in the 10-K filing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 490.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.25 seconds, 4.00 sentences/sec\n",
      "Average Correctness Score: 0.00\n",
      "Downloaded PDF from: https://investors.3m.com/financials/sec-filings/content/0001558370-19-000470/0001558370-19-000470.pdf to pdf_documents\\3M_2018_10K.pdf\n",
      "Using existing database for document: 3M_2018_10K\n",
      "Querying Model now\n",
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027B1CF9A060>\n",
      "To determine the year end FY2018 net PPNE (Property, Plant, and Equipment) for 3M in USD billions, we can refer to the Balance Sheet provided in the 10-K filing for that year. Based on the information in the 10-K filing for 3M for FY2018, the net Property, Plant, and Equipment (PP&E) value was approximately $9.678 billion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.51 seconds, 1.96 sentences/sec\n",
      "Average Correctness Score: 0.02\n",
      "Downloaded PDF from: https://investors.3m.com/financials/sec-filings/content/0000066740-23-000014/0000066740-23-000014.pdf to pdf_documents\\3M_2022_10K.pdf\n",
      "Creating new database for document: 3M_2022_10K\n",
      "252\n",
      "Querying Model now\n",
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027B0A925DC0>\n",
      "To determine if 3M is a capital-intensive business based on FY2022 data, we would typically look at metrics such as capital expenditures, depreciation, and other relevant financial indicators in the company's financial statements. To provide an accurate answer, I would need access to 3M's FY2022 10-K filing to analyze the capital intensity of the business. If you can provide the specific details from the FY2022 10-K, I can assist in evaluating whether 3M is capital-intensive based on that data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 889.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.64 seconds, 1.56 sentences/sec\n",
      "Average Correctness Score: 0.70\n",
      "Downloaded PDF from: https://investors.3m.com/financials/sec-filings/content/0000066740-23-000014/0000066740-23-000014.pdf to pdf_documents\\3M_2022_10K.pdf\n",
      "Using existing database for document: 3M_2022_10K\n",
      "Querying Model now\n",
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027B1CF8F2F0>\n",
      "In the 10-K filing provided, I couldn't find specific details explaining the drivers of the operating margin change for 3M as of FY2022. Operating margin is a useful metric for evaluating a company's operational efficiency and profitability. It is calculated as operating income divided by revenue and shows how much profit a company makes on each dollar of sales after accounting for variable costs of production.\n",
      "\n",
      "If there are no details provided in the filing regarding the operating margin change for 3M in FY2022, then I cannot provide a specific explanation based on the information available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 633.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.72 seconds, 1.39 sentences/sec\n",
      "Average Correctness Score: 0.09\n",
      "Downloaded PDF from: https://investors.3m.com/financials/sec-filings/content/0000066740-23-000014/0000066740-23-000014.pdf to pdf_documents\\3M_2022_10K.pdf\n",
      "Using existing database for document: 3M_2022_10K\n",
      "Querying Model now\n",
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027B15AF7B60>\n",
      "In 2022, excluding the impact of M&A, 3M's Safety and Industrial segment experienced a decline in organic local currency sales, which impacted the company's overall growth. The 10-K filing states that organic local currency sales in the Safety and Industrial segment were down 4.4% in 2022 compared to the prior year.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.49 seconds, 2.03 sentences/sec\n",
      "Average Correctness Score: 0.02\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
