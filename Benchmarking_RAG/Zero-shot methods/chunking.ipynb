{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coolk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "import base64\n",
    "from datasets import load_dataset\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import openai\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "from openai import OpenAI, ChatCompletion\n",
    "import uuid\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Ensure this import is after chromadb is fully imported\n",
    "try:\n",
    "    from chromadb.utils import embedding_functions\n",
    "except AttributeError as e:\n",
    "    print(f\"Error importing embedding_functions: {e}\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_url(url):\n",
    "    if url.lower().endswith('.pdf'):\n",
    "        return url  # Direct PDF URL\n",
    "    else:\n",
    "        parsed_url = urlparse(url)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        pdf_target = query_params.get('pdfTarget', [None])[0]\n",
    "\n",
    "        if pdf_target:\n",
    "            pdf_url = base64.b64decode(pdf_target).decode('utf-8')\n",
    "            return pdf_url\n",
    "        else:\n",
    "            raise ValueError(\"No valid PDF URL found in the provided URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, save_path):\n",
    "    try:\n",
    "        pdf_url = extract_pdf_url(url)\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        if not(os.path.exists(save_path)):\n",
    "            with open(save_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(f\"Downloaded PDF from: {pdf_url} to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading PDF: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "def truncate_chunks(sentences, max_tokens = 8192):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.encode(sentence))\n",
    "        if current_length + sentence_length > max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_combine(lst, sublist_size, sublist_func):\n",
    "    sublists = [lst[i:i + sublist_size] for i in range(0, len(lst), sublist_size)]\n",
    "    processed_sublists = [sublist_func(sublist) for sublist in sublists]\n",
    "    combined_result = [item for sublist in processed_sublists for item in sublist]\n",
    "    return combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up inputs into the embedding function\n",
    "import re  \n",
    "def ensure_utf8(strings):\n",
    "    cleaned_strings = []\n",
    "    for string in strings:\n",
    "        clean_string = string.encode('utf-8', 'replace').decode('utf-8', 'replace')\n",
    "        clean_string = re.sub(r'<\\|.*?\\|>', '', clean_string)\n",
    "        clean_string = ' '.join(clean_string.split())\n",
    "        cleaned_strings.append(clean_string)\n",
    "    return cleaned_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "openai_api_key = \"\"\n",
    "def create_embeddings(documents, openai_api_key, max_tokens = 8192):\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=openai_api_key,\n",
    "        model_name=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    sentences = [doc.page_content for doc in documents]\n",
    "    new_sentences = []\n",
    "    page_connection = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(tokenizer.encode(sentence)) > (max_tokens-50):\n",
    "            new_sentences = nltk.sent_tokenize(sentence)\n",
    "            new_chunks = truncate_chunks(new_sentences, max_tokens=max_tokens)\n",
    "            for chunk in new_chunks:\n",
    "                new_sentences.append(chunk)\n",
    "                page_connection.append(chunk)\n",
    "        else:\n",
    "            new_sentences.append(sentence)\n",
    "            page_connection.append(sentence)\n",
    "    new_sentences = ensure_utf8(new_sentences)\n",
    "    new_sentences = [x.replace(\"\\n\", \" \").replace('  ', ' ') for x in new_sentences]\n",
    "    vectors = process_and_combine(new_sentences, 2047, openai_ef)\n",
    "    vectors_pages = list(zip(vectors, page_connection))\n",
    "    return vectors_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata if metadata else {}\n",
    "\n",
    "class AgenticChunker:\n",
    "    def __init__(self, openai_api_key):\n",
    "        self.chunks = {}\n",
    "        self.id_truncate_limit = 5\n",
    "        self.generate_new_metadata_ind = True\n",
    "        self.print_logging = True\n",
    "        openai.api_key = openai_api_key\n",
    "\n",
    "    def add_propositions(self, propositions):\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "\n",
    "    def add_proposition(self, proposition):\n",
    "        if self.print_logging:\n",
    "            print(f\"\\nAdding: '{proposition}'\")\n",
    "        if len(self.chunks) == 0:\n",
    "            if self.print_logging:\n",
    "                print(\"No chunks, creating a new one\")\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "        chunk_id = self._find_relevant_chunk(proposition)\n",
    "        if chunk_id:\n",
    "            if self.print_logging:\n",
    "                print(f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
    "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
    "        else:\n",
    "            if self.print_logging:\n",
    "                print(\"No chunks found\")\n",
    "            self._create_new_chunk(proposition)\n",
    "\n",
    "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
    "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "        if self.generate_new_metadata_ind:\n",
    "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
    "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
    "\n",
    "    def _update_chunk_summary(self, chunk):\n",
    "        prompt = \"\"\"\n",
    "        You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\n",
    "        A new proposition was just added to one of your chunks, generate a brief 1-sentence summary for the chunk.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Chunk's propositions:\\n{'\\n'.join(chunk['propositions'])}\\n\\nCurrent chunk summary:\\n{chunk['summary']}\"}\n",
    "        ]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=messages,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        new_chunk_summary = response.choices[0].message['content'].strip()\n",
    "        return new_chunk_summary\n",
    "\n",
    "    def _update_chunk_title(self, chunk):\n",
    "        prompt = \"\"\"\n",
    "        You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\n",
    "        A new proposition was just added to one of your chunks, generate a brief updated chunk title.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Chunk's propositions:\\n{'\\n'.join(chunk['propositions'])}\\n\\nChunk summary:\\n{chunk['summary']}\\n\\nCurrent chunk title:\\n{chunk['title']}\"}\n",
    "        ]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=messages,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        updated_chunk_title = response.choices[0].message['content'].strip()\n",
    "        return updated_chunk_title\n",
    "\n",
    "    def _create_new_chunk(self, proposition):\n",
    "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit]\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id': new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title': new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index': len(self.chunks)\n",
    "        }\n",
    "        if self.print_logging:\n",
    "            print(f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
    "\n",
    "    def _get_new_chunk_summary(self, proposition):\n",
    "        prompt = \"\"\"\n",
    "        You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\n",
    "        Generate a brief 1-sentence summary for the new chunk.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Proposition:\\n{proposition}\"}\n",
    "        ]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=messages,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        new_chunk_summary = response.choices[0].message['content'].strip()\n",
    "        return new_chunk_summary\n",
    "\n",
    "    def _get_new_chunk_title(self, new_chunk_summary):\n",
    "        prompt = \"\"\"\n",
    "        You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\n",
    "        Generate a brief title for the new chunk.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Chunk summary:\\n{new_chunk_summary}\"}\n",
    "        ]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=messages,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        new_chunk_title = response.choices[0].message['content'].strip()\n",
    "        return new_chunk_title\n",
    "\n",
    "    def _find_relevant_chunk(self, proposition):\n",
    "        # This is a placeholder for the actual logic to find a relevant chunk\n",
    "        return None\n",
    "\n",
    "    def get_chunks(self, get_type='dict'):\n",
    "        if get_type == 'dict':\n",
    "            return self.chunks\n",
    "        if get_type == 'list_of_strings':\n",
    "            chunks = [\" \".join(chunk['propositions']) for chunk in self.chunks.values()]\n",
    "            return chunks\n",
    "\n",
    "    def pretty_print_chunks(self):\n",
    "        print(f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
    "            print(f\"Chunk ID: {chunk_id}\")\n",
    "            print(f\"Summary: {chunk['summary']}\")\n",
    "            print(f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                print(f\"    -{prop}\")\n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, method=\"character\", chunk_size=100, chunk_overlap=0):\n",
    "    if method == \"character\":\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    elif method == \"recursive\":\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    elif method == \"semantic\":\n",
    "        text_splitter = SemanticChunker(OpenAIEmbeddings(openai_api_key=\"\"), breakpoint_threshold_type=\"percentile\")\n",
    "        documents = text_splitter.create_documents([text])\n",
    "        return documents\n",
    "    elif method == \"agentic\":\n",
    "        ac = AgenticChunker(openai_api_key=\"\")\n",
    "        sentences = text.split('.')\n",
    "        ac.add_propositions(sentences)\n",
    "        chunks = ac.get_chunks(get_type='list_of_strings')\n",
    "        documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown chunking method\")\n",
    "    return text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings_in_chroma(documents, vectors, collection_name=\"Finance_bench_documents\"):\n",
    "    client = chromadb.Client(Settings())\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    for i, (vector, page_connection) in enumerate(vectors):\n",
    "         collection.upsert(f\"id_{i}\", vector, {\"sentence\": page_connection})\n",
    "local_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question, collection_name=\"FinanceBench_Embeddings\", api_key=\"\"):\n",
    "    client = chromadb.Client(Settings())\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "            api_key=openai_api_key,\n",
    "            model_name=\"text-embedding-ada-002\"\n",
    "        )\n",
    "    collection = client.get_collection(collection_name, embedding_function=openai_ef)\n",
    "    top_vectors = collection.query(query_texts=[question], n_results=10)\n",
    "    rag_text = \"\"\n",
    "    for metadata_pair in top_vectors['metadatas']:\n",
    "        for metadata in metadata_pair:\n",
    "            rag_text += f\"{metadata['sentence']}\\n\"\n",
    "    if len(rag_text) > 128000:\n",
    "        rag_text = rag_text[:128000]\n",
    "\n",
    "    template = \"\"\"You are a financial chatbot trained to answer questions based on the information provided in 10-K\n",
    "    documents. Your responses should be directly sourced from the content of these documents. When asked\n",
    "    a question, ensure that your answer is explicitly supported by the text in the 10-K filing, and do not\n",
    "    include any external information, interpretations, or assumptions not clearly stated in the document. If\n",
    "    a question pertains to financial data or analysis that is not explicitly covered in the 10-K filing provided,\n",
    "    respond by stating that the information is not available in the document. Your primary focus should\n",
    "    be on accuracy, specificity, and adherence to the information in 10-K documents, particularly regarding\n",
    "    financial statements, company performance, and market position.\"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    query_prompt = f\"Question: {template}. Relevant document information: {rag_text}\"\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\":\"user\", \"content\":query_prompt},\n",
    "    ]\n",
    "    openai_client = OpenAI(api_key=openai_api_key)\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages = messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    similarity_score = cosine_sim[0][0]\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bertscore(candidate, reference):\n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    return P.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "def evaluate_llm_responses(question, model_answer, reference_answer):\n",
    "    evaluation_scores = []\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an evaluator that scores responses based on correctness.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Evaluate the following response against the reference answer. Assign a score between 0 and 1 based on correctness and provide a brief justification.\n",
    "\n",
    "        Question: {question}\n",
    "        Response: {model_answer}\n",
    "        Reference Answer: {reference_answer}\n",
    "\n",
    "        Score (0 to 1):\n",
    "        Justification:\n",
    "        \"\"\")\n",
    "    ]\n",
    "    response = local_llm(messages=messages)  # Using local_llm for evaluation\n",
    "    evaluation_text = response.content.strip()\n",
    "    \n",
    "    try:\n",
    "        score_line = evaluation_text.split('\\n')[0]\n",
    "        score = float(score_line.split(':')[1].strip())\n",
    "        evaluation_scores.append(score)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing score: {e}\")\n",
    "        evaluation_scores.append(0.0)\n",
    "\n",
    "    average_score = sum(evaluation_scores) / len(evaluation_scores) if evaluation_scores else 0\n",
    "    print(f'Average Correctness Score: {average_score:.2f}')\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chunking_techniques(df, openai_api_key):\n",
    "    chunking_methods = [\"character\", \"recursive\", \"semantic\"]\n",
    "    # chunking_methods = [\"agentic\", \"character\", \"recursive\", \"semantic\"]\n",
    "    results = []\n",
    "\n",
    "    for method in chunking_methods:\n",
    "        print(f\"Evaluating chunking method: {method}\")\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            download_dir = \"pdf_documents\"\n",
    "            os.makedirs(download_dir, exist_ok=True)\n",
    "            pdf_url = row['doc_link']\n",
    "            doc_name = row['doc_name']\n",
    "            question = row['question']\n",
    "            ref_answer = row['answer']\n",
    "            ref_context = row['evidence_text']\n",
    "\n",
    "            doc_path = os.path.join(download_dir, f\"{doc_name}.pdf\")\n",
    "\n",
    "            #save_path = f\"downloads/{row['financebench_id']}.pdf\"\n",
    "            download_pdf(pdf_url, doc_path)\n",
    "\n",
    "            text = extract_text_from_pdf(doc_path)\n",
    "            documents = chunk_text(text, method=method)\n",
    "            vectors = create_embeddings(documents, openai_api_key)\n",
    "\n",
    "\n",
    "            store_embeddings_in_chroma(documents, vectors, collection_name=f\"Finance_bench_{method}\")\n",
    "\n",
    "            model_answer = rag(question, collection_name=f\"Finance_bench_{method}\", api_key=openai_api_key)\n",
    "\n",
    "            cosine_similarity_score = calculate_cosine_similarity(model_answer, ref_context)\n",
    "            bert_score_value = calculate_bertscore(model_answer, ref_context)\n",
    "            llm_eval = evaluate_llm_responses(question, model_answer, ref_context)\n",
    "\n",
    "            results.append({\n",
    "                \"doc_name\" : doc_name,\n",
    "                \"method\": method,\n",
    "                \"question\": question,\n",
    "                \"ref_answer\": ref_answer,\n",
    "                \"model_answer\": model_answer,\n",
    "                \"cosine_similarity\": cosine_similarity_score,\n",
    "                \"bert_score\": bert_score_value,\n",
    "                \"llm_eval\": llm_eval\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating chunking method: character\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating chunking method: character\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 106.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.58 seconds, 0.18 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 11.18 seconds, 0.09 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 116.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.12 seconds, 0.32 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 133.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.60 seconds, 0.28 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 125.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.75 seconds, 0.21 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating chunking method: recursive\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating chunking method: recursive\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 33.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.38 seconds, 0.23 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 155.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.51 seconds, 0.18 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.39 seconds, 0.72 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.49 seconds, 0.29 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.20</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.20\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 283.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.43 seconds, 0.29 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating chunking method: semantic\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating chunking method: semantic\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.34 seconds, 0.23 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 189.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.19 seconds, 0.24 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 308.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.75 seconds, 0.21 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 315.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.99 seconds, 0.20 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.50</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m0.50\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 998.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.81 seconds, 1.24 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Correctness Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Correctness Score: \u001b[1;36m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">      method  cosine_similarity  bert_score  llm_eval\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  character           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.096186</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.780959</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  recursive           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.161767</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.799923</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>   semantic           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.346303</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.797074</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.30</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "      method  cosine_similarity  bert_score  llm_eval\n",
       "\u001b[1;36m0\u001b[0m  character           \u001b[1;36m0.096186\u001b[0m    \u001b[1;36m0.780959\u001b[0m      \u001b[1;36m0.00\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m  recursive           \u001b[1;36m0.161767\u001b[0m    \u001b[1;36m0.799923\u001b[0m      \u001b[1;36m0.04\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m   semantic           \u001b[1;36m0.346303\u001b[0m    \u001b[1;36m0.797074\u001b[0m      \u001b[1;36m0.30\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def main():\n",
    "    dataset = load_dataset(\"PatronusAI/financebench\")\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    test = df.head(5)\n",
    "    download_dir = \"documents_QE\"\n",
    "    if not(os.path.exists(download_dir)):\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "    return test\n",
    "\n",
    "\n",
    "    \n",
    "test_data = main()\n",
    "results_df = evaluate_chunking_techniques(test_data, openai_api_key = \"\")\n",
    "\n",
    "#Save results to a CSV file\n",
    "results_df['model_answer'] = results_df['model_answer'].str.replace('\\n', '<newline>').replace(',', '<comma>')\n",
    "results_df['ref_answer'] = results_df['ref_answer'].str.replace('\\n', '<newline>').replace(',', '<comma>')\n",
    "results_df.to_csv(\"chunking_evaluation_results.csv\", index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(results_df):\n",
    "    metrics = results_df.groupby('method').agg({\n",
    "        'cosine_similarity': 'mean',\n",
    "        'bert_score': 'mean',\n",
    "        'llm_eval': 'mean'\n",
    "    }).reset_index()\n",
    "    return metrics\n",
    "\n",
    "metrics = calculate_metrics(results_df)\n",
    "print(metrics)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
