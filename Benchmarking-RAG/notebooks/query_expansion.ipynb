{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coolk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "from pypdf import PdfReader\n",
    "\n",
    "import os\n",
    "root = os.getcwd().split('Anote')[0] + 'Anote'\n",
    "path_to_pdf_storage = f'{root}/Benchmarking_RAG/documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_url(url):\n",
    "    \"\"\"\n",
    "    Extracts the actual PDF URL from the given URL.\n",
    "    Decodes it from base64 if necessary.\n",
    "    \"\"\"\n",
    "    if url.lower().endswith('.pdf'):\n",
    "        return url  # Direct PDF URL\n",
    "    else:\n",
    "        parsed_url = urlparse(url)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        pdf_target = query_params.get('pdfTarget', [None])[0]\n",
    "\n",
    "        if pdf_target:\n",
    "            pdf_url = base64.b64decode(pdf_target).decode('utf-8')\n",
    "            return pdf_url\n",
    "        else:\n",
    "            raise ValueError(\"No valid PDF URL found in the provided URL\")\n",
    "\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from a given URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_url = extract_pdf_url(url)\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        if not(os.path.exists(save_path)):\n",
    "            with open(save_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            print(f\"Downloaded PDF from: {pdf_url} to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading PDF: {e}\")\n",
    "\n",
    "def get_pages_from_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    pages_text = []\n",
    "    for idx, page in enumerate(reader.pages):\n",
    "        pages_text.append(page.extract_text())\n",
    "    return pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_chroma():\n",
    "    client = chromadb.PersistentClient(path=f'{path_to_pdf_storage}/chromadb.db')\n",
    "    collection_name = \"FinanceBench_Embeddings\"\n",
    "    if not(collection_name in [c.name for c in client.list_collections()]):\n",
    "        collection = client.create_collection(name=collection_name)#, embedding_function=embedding_function)\n",
    "    else:\n",
    "        print('already exists - returning')\n",
    "        return\n",
    "\n",
    "    files = glob.glob(f'{path_to_pdf_storage}/*.pdf')\n",
    "    files = [x for x in files if not(\".pdf.pdf\" in x)]\n",
    "    print('files is:', files)\n",
    "    for idx, path in enumerate(files):\n",
    "        pages = get_pages_from_pdf(path)\n",
    "        my_ids=[f'{str(idx)}_{x[0]}' for x in list(enumerate(pages))]\n",
    "        collection.add(\n",
    "            documents= pages,\n",
    "            ids=my_ids,\n",
    "            metadatas=[{'doc_path':path}]*len(my_ids),\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_vectordb_from_pdf(pdf_path, openai_api_key, batch_size=100):\n",
    "# Extract text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = text.split('\\n')\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]  # Remove empty sentences\n",
    "    \n",
    "    # Initialize OpenAI Embedding Function\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=openai_api_key,\n",
    "        model_name=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    \n",
    "    # Batch processing for embeddings\n",
    "    vectors = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        if len(batch) > 0:  # Ensure batch is not empty\n",
    "            batch_vectors = openai_ef(batch)\n",
    "            vectors.extend(batch_vectors)\n",
    "    \n",
    "    # Store vectors in Chroma vector database\n",
    "    client = chromadb.Client(Settings())\n",
    "    collection_name = \"Finance_bench_documents\"\n",
    "    collection = client.get_or_create_collection(name= collection_name) \n",
    "    # if client.has_collection(collection_name):\n",
    "    #     collection = client.get_collection(collection_name)\n",
    "    # else:\n",
    "    #     collection = client.create_collection(collection_name)\n",
    "    \n",
    "    for i, (sentence, vector) in enumerate(zip(sentences, vectors)):\n",
    "        collection.add(f\"id_{i}\", vector, {\"sentence\": sentence})\n",
    "    \n",
    "    print(f\"Stored {len(sentences)} vectors in the Chroma vector database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_context(query, openai_api_key):\n",
    "    ai_model = \"gpt-4o\"\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    system_prompt =\"\"\"You are a financial chatbot trained to answer financial questions to the absolute best of your ability. Your primary focus should be on accuracy, specificity, and correctness, particularly relating to financial statements, company performance, and market position. Please answer each question with total accuracy, performing all necessary calcualtions without skipping or simplying any steps along the way. If you do not have enough information to answer a question, please make whatever reasonable assumptions are necessary and provide a full and complete answer.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\":\"user\", \"content\":f\"Question: {query}\"},\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages = messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_initial_response(query, initial_response, openai_api_key):\n",
    "    collection_name=\"FinanceBench_Embeddings\"\n",
    "    chroma_client = chromadb.PersistentClient(path=f'{path_to_pdf_storage}/chromadb.db')\n",
    "    collection = chroma_client.get_collection(collection_name)\n",
    "    rag_text = '\\n\\n'.join(collection.query(query_texts=[query, initial_response], n_results=10)['documents'][0])\n",
    "\n",
    "    system_prompt = \"\"\"You are a financial chatbot trained to answer questions based on the information provided in 10-Ks and other financial\n",
    "    documents. Your responses should be directly sourced from the content of these documents. When asked\n",
    "    a question, ensure that your answer is explicitly supported by the text in the document, and do not\n",
    "    include any external information, interpretations, or assumptions not clearly stated in the document. If\n",
    "    a question pertains to financial data or analysis that is not explicitly covered in the document text filing provided,\n",
    "    respond by stating that the information is not available in the document. Your primary focus should\n",
    "    be on accuracy, specificity, and adherence to the information in the documents, particularly regarding\n",
    "    financial statements, company performance, and market position.\"\"\"\n",
    "\n",
    "    query_prompt = f\"Question: {query}. Relevant document information: {rag_text}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\":\"user\", \"content\":query_prompt},\n",
    "    ]\n",
    "    openai_client = OpenAI(api_key=openai_api_key)\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages = messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_responses(question, model_answer, refrence_answer):\n",
    "    for question in question:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that provides concise and accurate answers.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        response = model_answer\n",
    "\n",
    "    evaluation_scores = []\n",
    "    for i in range(len(response)):\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Evaluate the following response against the reference answer. Assign a score between 0 and 1 based on correctness and provide a brief justification.\n",
    "\n",
    "        Question: {question}\n",
    "        Response: {model_answer}\n",
    "        Reference Answer: {refrence_answer}\n",
    "\n",
    "        Score (0 to 1):\n",
    "        Justification:\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an evaluator that scores responses based on correctness.\"},\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "        ]\n",
    "        evaluation_response = get_assistant_response(messages)\n",
    "\n",
    "        evaluation_text = evaluation_response.strip()\n",
    "        try:\n",
    "            score_line = evaluation_text.split('\\n')[0]\n",
    "            score = float(score_line.split(':')[1].strip())\n",
    "            evaluation_scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing score: {e}\")\n",
    "            evaluation_scores.append(0.0)\n",
    "\n",
    "    average_score = sum(evaluation_scores) / len(evaluation_scores) if evaluation_scores else 0\n",
    "    print(f'Average Correctness Score: {average_score:.2f}')\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chat_history(messages):\n",
    "    for message in messages:\n",
    "        print(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "\n",
    "def get_assistant_response(messages):\n",
    "  client = OpenAI(api_key = openai_api_key)\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages],\n",
    "  )\n",
    "\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(openai_api_key):\n",
    "    dataset = load_dataset(\"PatronusAI/financebench\")\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    download_dir = \"documents_QE\"\n",
    "    if not(os.path.exists(download_dir)):\n",
    "           os.makedirs(download_dir, exist_ok=True)\n",
    "           print(\"making directory\")\n",
    "           df.apply(lambda x: download_pdf(x.doc_link, os.path.join(download_dir, f\"{x.doc_name}.pdf\")), axis=1) #download all the pdfs\n",
    "           print(\"downloaded all pdfs\")\n",
    "           seed_chroma()\n",
    "           print(\"created Chroma DB\")\n",
    "\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        query = row['question']\n",
    "        # refrence_answer = row['answer']\n",
    "        # doc_name = row['doc_name']\n",
    "        # doc_link = row['doc_link']\n",
    "\n",
    "        initial_response = no_context(query, openai_api_key)\n",
    "        final_response = with_initial_response(query, initial_response, openai_api_key)\n",
    "\n",
    "        print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main(\u001b[43mopenai_api_key\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai_api_key' is not defined"
     ]
    }
   ],
   "source": [
    "main(openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bert_score import score\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    # Create a TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the two texts\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    # Extract the similarity score\n",
    "    similarity_score = cosine_sim[0][0]\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "def calculate_bertscore(candidate, reference):\n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    return P.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test, openai_api_key):\n",
    "    results_dl = []\n",
    "    for index, row in test.iterrows():\n",
    "        download_dir = \"pdf_documents\"\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        doc_link = row['doc_link']\n",
    "        doc_name = row['doc_name']\n",
    "        query = row['question']\n",
    "        ref_answer = row['answer']\n",
    "        ref_context = row['evidence_text']\n",
    "        \n",
    "        doc_path = os.path.join(download_dir, f\"{doc_name}.pdf\")\n",
    "\n",
    "        download_pdf(doc_link, doc_path)\n",
    "        create_chroma_vectordb_from_pdf(doc_path, openai_api_key)\n",
    "        print(\"Querying Model now\")\n",
    "        initial_response = no_context(query, openai_api_key)\n",
    "        final_response = with_initial_response(query, initial_response, openai_api_key)\n",
    "\n",
    "\n",
    "        #Evaluation\n",
    "        cosine_similarity_score = calculate_cosine_similarity(final_response, ref_answer)\n",
    "        bert_score = calculate_bertscore(final_response, ref_answer)\n",
    "        llm_eval = evaluate_llm_responses(query, final_response, ref_answer)\n",
    "\n",
    "\n",
    "        results_dl.append({\n",
    "            'doc_name': doc_name,\n",
    "            'question': query,\n",
    "            'ref_answer': ref_answer,\n",
    "            'final_response': final_response,\n",
    "            'cosine_similarity': cosine_similarity_score,\n",
    "            'bert_score': bert_score,\n",
    "            'llm_eval': llm_eval\n",
    "        })\n",
    "    results_df = pd.DataFrame(results_dl)\n",
    "    results_df.to_csv('query_expansion_results.csv', index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"PatronusAI/financebench\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "test = df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluate_model(test, \u001b[43mopenai_api_key\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai_api_key' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_model(test, openai_api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
